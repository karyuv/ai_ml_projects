{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is Supervised Learning?\n",
    "Supervised learning is when you already know the label of the target variable. It is of two types: regression and classification. Regression is when the target variable is continuous and classification is when the target 8variable is in form of categories or discrete values.\n",
    "\n",
    "For Example:\n",
    "**Regression**: House Price prediction based on area, number of rooms, lawn, pool, etc. \n",
    "**Classification**: Predicting whether a person will be diabetic in the future or not based on BP, glucose, insulin, etc.\n",
    "\n",
    "As you can see here, it is already known the target variable is the price in case of regression and whether the person is diabetic or not in case of classification. So we know what we want to predict.\n",
    "\n",
    "2. When should we use Precision or Recall as model performance evaluation criteria?\n",
    "Precision should be used when you want to minimize False Positives i.e. one wants at least negatives should not be predicted as positives. Also, in cases where the loss of resources is high.\n",
    "\n",
    "Recall should be used when you want to minimize False Negatives, i.e. one wants at least positives should not be predicted as negatives. Also, in cases where the loss of opportunity is high.\n",
    "\n",
    "3. What is misclassification?\n",
    "Misclassification occurs when values are predicted incorrectly or the model assigns the observation to a different class instead of the class it should be in. For example, for observation, the actual label is class 0 but the model predicts this observation as class 1.\n",
    "\n",
    "4. Why confusion matrix is inverted in the hands-on notebook and it is difficult to identify TP, TN, FP, and FN?\n",
    "Generally, in theory, or while teaching confusion matrix many sources keep the predicted labels are on the y-axis whereas the actual labels are on the x-axis. \n",
    "\n",
    "But in the implementation, it can be different depending upon the library we are using.\n",
    "Sklearn follows an approach of Actual labels on the x-axis and Predicted labels on the y-axis.\n",
    "\n",
    "Let us understand how to identify TP, TN, FP, and FN in a confusion matrix through an example:\n",
    "\n",
    "Let's say we are trying to predict a person is diabetic (class 1) or not (class 0). \n",
    "\n",
    "True Positives (TP):  A person has diabetes and the model predicted that person is diabetic.\n",
    "True Negatives (TN): A person doesn't have diabetes and the model predicted that person doesn't have diabetes.\n",
    "False Positives (FP): A person doesn't have diabetes but the model predicted that person has diabetes.\n",
    "False Negatives (FN): The person has diabetes but the model predicted that person doesn't have diabetes. \n",
    "Now based on the actual label and predicted label you can identify the TP, TN, FP, and FN.\n",
    "\n",
    "\n",
    "5. When do we label encode and create dummy variables for categorical levels?\n",
    "We generally prefer label encoding when there is a sense of order on the values, for example, let's say a variable has values bad, good, very good in such a case we know that there is an order and we can encode them as 1,2,3 respectively.\n",
    "\n",
    "But let's say the values are red, blue, green in this case there is no definite order in values and hence creating dummy variables would be a better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid curves:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Curves which look like a S.\n",
    "- Generated by a sigmoid function:\n",
    "    - logisitic regression (also callled as **logit** function) (the most popular for classification problem)\n",
    "    - probit regression.\n",
    "- odds ratio: y/1-y (success divided by not success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log loss:\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Minimizing log loss (also called as entropy) gives better model in classification.\n",
    "- Bigger log loss eror leads to poorly predicting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification error rate:**\n",
    " - It is ratio of (FP+FN)/(TP+FP+FN+TN)\n",
    " - FN and TN is false negatives and True Negatives.\n",
    " \n",
    "**True positive rate:**\n",
    "-  it is ratio of TP/(TP+FN)\n",
    "- also called as Recall or Senstivity.\n",
    "\n",
    "**True negative rate:**\n",
    "-  it is ratio of TN/(TN+FP)\n",
    "- also called as Specificity\n",
    "\n",
    "**ROC curve**\n",
    "- It is a plot between FPR (False Postive rate) and TPR (True positive rate). \n",
    "- FPR is FP/FP+TN\n",
    "- TPR is TP/(FN+TP)\n",
    "\n",
    "**GIDI coefficient**:\n",
    "- derived from the ROC plot.\n",
    "- it is (2 x Area_under_the_curve)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1 score**:\n",
    " - it is ratio of 2 x precision x recall/(precision+recall)\"\n",
    " - it is represent both the precision and recall.\n",
    "\n",
    "**Accuracy:**\n",
    "- it is ratio of TN+TP/(TN+TP+FP+FN)\n",
    "\n",
    "**Recall:**\n",
    "- it is ratio of TP/(TP+FN)\n",
    "\n",
    "**Precision:**\n",
    "- it is ratio of TP/(TP+FP)\n",
    "\n",
    "**Useful link**: https://medium.com/@erika.dauria/accuracy-recall-precision-80a5b6cbd28d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
