{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Hierarchical Clustering and Dimensionality Reduction techniques - PCA\n",
    " \n",
    "## PCA does not help with feature importance finding, but transforms the columns\n",
    "Using PCA, the columns are completely changed. PCA finds a new relationship between the columns. So, we use it to get a higher score. If the focus is to find feature importance, then PCA fails as it completely transforms the columns and new columns will not mean the same as the older ones, so we cannot name them. Feature importance after PCA will make no sense at all.\n",
    "\n",
    "After transforming columns using PCA we try to take the minimum number of columns through which we can achieve the maximum score.\n",
    "\n",
    "## Distance and clustering\n",
    "There is no single distance that will give the best results with all data and all problem statements. The type of distance you use depends on the data and the problem statement. Generally normalized euclidean distance is used. However, when data size is very large and high dimensional, Manhattan distance is found to perform better computationally. The type of distance to use is decided by the problem at hand.\n",
    "\n",
    "\n",
    "## Kmeans vs Hierachial clustering\n",
    "K-means algorithm is used when it is already known in advance how many clusters have to be formed, also k-means is suitable if your data is well separated into spherical-like clusters. On the other hand, hierarchical clustering is density-based clustering in which nearby points are joined to form clusters. It gives you a dendrogram from which you can figure out how many clusters should be formed. Hierarchical clustering is computationally expensive so it will not perform well when data size is very very big.\n",
    "\n",
    "## Correlation, clustering and computational efficiency\n",
    "First of all, you have to decide which variables you should be used for clustering, Once you have done that, it is better to choose only those variables among them, such that no two variables have a correlation of more than 0.7 (magnitude).\n",
    "\n",
    "Correlation does not have a negative impact on clustering but removing correlated variables helps to reduce the dimension and can be computationally efficient when you have a very large number of observations.\n",
    "\n",
    "\n",
    "## Choosing the optimal number of clusters in hierachial clustering\n",
    "The optimal number of clusters from a dendrogram can be obtained by deciding where to cut the cluster tree. Generally, the cluster tree is cut where dendrogram height is maximum as it corresponds to distinct and homogeneous clusters. However, one should also do cluster profiling and check if the cluster profiles are meaningful and have variability, for which domain knowledge is needed.\n",
    "\n",
    "If the chosen number of clusters does not seem meaningful from the cluster profiles or does not align with domain knowledge, then one should choose different values of k (number of clusters) and repeat the process until the cluster profiles obtained are meaningful and align with the domain knowledge.\n",
    "\n",
    "\n",
    "## Notes from the lecture\n",
    "\n",
    "- Most common used distance is the euclidean distance.\n",
    "- Hierachial clustering: Connectivity based clustering:\n",
    "   - Check the relationship between different pairs of points.\n",
    "- Scaling\n",
    "  - scale the features (so all of them are under 1 scale)\n",
    "  - then find the euclidean distance.\n",
    "- Then after scaling, then find the 2 closest points.\n",
    "- then find those corresponding entries and see if it makes sense (meaning they should have a similar pattern).\n",
    "- then with the 2 closest points, then build the dendogram tree bottom up.\n",
    "- Distance between the clusters: different methods:\n",
    "   - Single Linkage: Min distance between the clusters (nearest neighbors)\n",
    "   - Complete linkage: Max distance between the cluster (the farthest distance).\n",
    "   - Average Linkage: Average of all the distance between different pairs of points between 2 clusters.\n",
    "   - **Centroid method** (most commonly used): Combine cluster with minimum distance between the centroids of the 2 clusters.\n",
    "   - ward's method\n",
    "\n",
    "## Divisive and agglomerative clustering\n",
    "- Divisive, or top-down, clustering method clusters data points by assigning all the observations to a single cluster and then partitioning the cluster into two least similar clusters. \n",
    "- Agglomerative, or bottom-up, clustering method clusters data points by assigning each observation to its own cluster, computing the similarity (e.g., distance) between each of the clusters, and joining the two most similar clusters repetitively till we have one large cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
