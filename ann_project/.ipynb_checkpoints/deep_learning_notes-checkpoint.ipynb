{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "- Deep learning is-inspired by our brain. Where neuron fires information between them.\n",
    "- Use cases:\n",
    "    - Lots of industries use it: Sephora.\n",
    "    - Google deep mind made the best possible go player.\n",
    "    - Tesla uses deep learning \n",
    "    - Google translate\n",
    "\n",
    "- Artifical neural networks:\n",
    "  - Input: mileage of a car\n",
    "  - Output: prediction of cost of the car.\n",
    "  - process of going from left to right of the network (is called **forward propogation**) and the opposite is called **backword propogation**.\n",
    "  \n",
    "- Google colab:\n",
    "  - we will be using google colab to train the neural networks\n",
    "  - google colab uses the google's hardware gpu, tpu (tensor processing unit) and cpu to work on the dataset\n",
    "\n",
    "- Tensors:\n",
    "  - DS of different dimensions\n",
    "  - vector, matrix, cube are all examples of tensors.\n",
    "\n",
    "- Tensors and deep learning:\n",
    "   - Tensors are the backbone of deep learning\n",
    "\n",
    "- Deep learning:\n",
    "   - Special type of ANNs.\n",
    "   - Built by Frank rosenblack (the perceptron)\n",
    "   - but limitations are there:\n",
    "     - excitement for deep learning started to die\n",
    "     - but they focused on decision trees etc.\n",
    "     - But hidden layers brought back the promise of deep learning\n",
    "     - Hidden layers can have any number of neurons we want.\n",
    "   - Imagenet, Alexnet and Resnet are great examples of hidden layer design success.\n",
    "\n",
    "- Activation functions:\n",
    "   - can create outputs than just straight lines. like sigmoid function (which gives output from 0 to 1) or a softmax sigmoid.\n",
    "   \n",
    "- Gradient descent:\n",
    "   - what it is ?:\n",
    "      - assign random weights\n",
    "      - calculate the loss\n",
    "      - take a step in the direction with the steepest gradient\n",
    "      - calucate the new loss. (and estimate the learning rate)\n",
    "      - update the random weights (in the 1st step) and redo the steps above (this is backpropagation)\n",
    "    - In a nutshell, optimize the paramters in such a way that for a the loss function gets reduced to a minimum (based on the number of steps defined). So in a iterative fashion, keep guessing the parameters, untill the loss function (cross entropy is the commonly used method) is the least. Then when arrived to the least loss function, cache the variables which got us there.\n",
    "    - A lot of calculus is involved.\n",
    "       - Power rule: d(x^n)= n*x^(n-1)\n",
    "       - Chain rule\n",
    "      \n",
    "     - definition: Gradient Descent is an optimization technique that is used to improve deep learning and neural network-based models by minimizing the loss.\n",
    "\n",
    "\n",
    "# Deep learning and gradient descent optimization:\n",
    "\n",
    "Gradient desecent optimization is the core algorithm for deep learning. These are the factors (or **hyper parameters**), which affect how the gradient descent performs:\n",
    "\n",
    " - Epocs:\n",
    "    -  the amount of time the training data is fed to the gradient descent process.\n",
    "    - Advantage: the more times you train, the further close you go to reach the gradient descent. \n",
    "    - Disadvantage: if there is a lot of data, the training time will be more, if the epoch is set higher.\n",
    "\n",
    "- Batch Size:\n",
    "   - Determines if to feed the whole training data or a part of it, to estimate the gradient descent.\n",
    "      - Batch gradient descent: Feed the whole training data to estimate the gradient descent.\n",
    "      - Mini-Batch gradient descent: A subset of the training data is fed to estimate the gradient descent.\n",
    "      - Stochastic gradient descent: Just one entry of the training data is fed to estimate the gradient descent.\n",
    "      - the number of steps taken are determined by **epoch** and **batch_size**.\n",
    "\n",
    "- Learning_rate:\n",
    "   - a constant which determines the size of the step taken.\n",
    "\n",
    "- Optimizers: (the kind of optimizer to use):\n",
    "\n",
    "    - Momentum optimizer:\n",
    "         - helps to not get stuck in local minima. \n",
    "         - estimates the step and also the direction we are travelling.\n",
    "    - Adagrad:\n",
    "         - Helps to avoid overshooting global minimum (hits the break)\n",
    "    - RMSprop:\n",
    "         - Helps adagrad from stopping competelty and helps to get to the final global minimum.\n",
    "    - Adam:\n",
    "         - Combines Momentum and RMSProp.\n",
    "\n",
    "# Normalizing\n",
    "- when assigning weights to the features, it has to be on the same scale. Hence z scale normalization is done.\n",
    "\n",
    "\n",
    "# How to select all the hyper-parameters:\n",
    "\n",
    "- The problem is there are a lot of parameters to tune in deep learning (like the ones discussed above).\n",
    "- Don't reinvent the wheeel (choose the hyper parameters from other projects which were succesfull)\n",
    "\n",
    "- Some rule of thumbs:\n",
    "    - 2 hidden layers\n",
    "    - 20 epocs\n",
    "    - Relu activation function\n",
    "    - Adam optimizer.\n",
    "\n",
    "- Plot the learning curve and observe:\n",
    "   - for example plot validation and train against the epoch data.\n",
    "\n",
    "- Start small and iterate:\n",
    "   - first get the basic stuff working\n",
    "   - write the necessary features:\n",
    "       - normalizing\n",
    "       - try different algorithms.\n",
    "- Dont strive for perfection:\n",
    "    - a model can never be 100% ready.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks - what is it?\n",
    "Neural Networks replicate the way humans learn, inspired by how the neurons in our brains fire, only much simpler.\n",
    "\n",
    "The most common Neural Networks consist of three network layers:\n",
    "\n",
    " - An input layer\n",
    "  - A hidden layer (this is the most important layer where feature extraction takes place, and adjustments are made to train faster and function better)\n",
    " - An output layer\n",
    " \n",
    "Each sheet contains neurons called “nodes,” performing various operations. Neural Networks are used in deep learning algorithms like CNN, RNN, GAN, etc.\n",
    "\n",
    "# What are hyper-parameters?\n",
    "With neural networks, you’re usually working with hyperparameters once the data is formatted correctly. A hyperparameter is a parameter whose value is set before the learning process begins. It determines how a network is trained and the structure of the network (such as the number of hidden units, the learning rate, epochs, etc.).\n",
    "\n",
    "\n",
    "\n",
    "# Microprocessor and NN\n",
    " - Microprocessors have gates (and, or etc)\n",
    " - gates have input notes and outputs.\n",
    " - Microprocessors are nothing more than gates (buried into it).\n",
    " - Basically a complex task is combinaion of simple gates put together.\n",
    " - advanced microprocessor today have 50 billion transistors.\n",
    " \n",
    "# Brain and NN\n",
    "   - Brain works through lots and lots of neuron (simples unit of brain)\n",
    "   - A brain has around 86 million neurons.\n",
    "   - An earthworm has 302 neurons.\n",
    "   - The whole goal of NN is to build the artificial brain.\n",
    "   - Fundamental block of NN is called **perceptron** (coined in 1940)\n",
    "   \n",
    "# Artificial Neuron  or perceptron\n",
    "- takes in weighted sum of inputs.\n",
    "- passes through non linear function (activation function) and an output is computed.\n",
    "- Basically NN is inpsired by the design of the brain, but does not model the brain itself.\n",
    "\n",
    "# Measure of a NN\n",
    "- The number of perceptrons does not correspond to the size of the NN.\n",
    "- The measure it by the number of connections, because they carry the weights. because it is the parameters to be optimized.\n",
    "- Nvidia has NN of 8 billion parameters and it took 10 days to train\n",
    "\n",
    "# One and multi-layer perceptron\n",
    "- when a perceptron gets trained, it basically tries to optimize the weights to get a y_cap, which should be closer to ground truth y.\n",
    "- so when we have multiple perceptrons, there are several weights to be optimzied, which produces a curve and used by other perceptrons.\n",
    "\n",
    "# MNIST\n",
    "- has 60k images, labelled from 0 to 9 (handwriten numbers labeeled from 0 to 9)\n",
    "- goal to computer recognize human labelled number.\n",
    "- input is 28 by 28 grid (784 inputs) of the hand writeen image and\n",
    "   output is 10 array (probability of number from 0 to 9)\n",
    "- hidden layer has 64 neurons\n",
    "- total number of perceptrons: (784*64(dense)+64(bias)) + 64*10(outputs) + 10 (bias)\n",
    "\n",
    "\n",
    "# Types of activation function\n",
    "Smooth functions are desirable for deep learning\n",
    "- step function (not smooth)\n",
    "- sigmoid (0 to 1)\n",
    "- Tanh (-1 to 1)\n",
    "- Relu (most commonly used)\n",
    "- Linear activation function (takes the input x and spits the same output x, good for passing data)\n",
    "- softmax (betterfor multi class clasification)\n",
    "\n",
    "- Sigmoid, Tanh and Relu are suitable for the hidden layers\n",
    "- Linear activation function used for regression based or the output layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands on\n",
    " - Data must not have any missing values (mode or median)\n",
    " - Also all the object variables must be hot encoded.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD with momentum\n",
    "- In convex optimization, anywhere the slope is always increasing.\n",
    "- It is very hard to optimize non convex functions\n",
    "    - Very high chance of getting caught in local minima.\n",
    "- If learning rate is too large, you will be oscillating a lot\n",
    "- if too small, takes longer time to converge.\n",
    "- This is an adhoc method (discovered through trail and error)\n",
    "- SGD is kind of falling process and adding momentum helps.\n",
    "- It accelerates the grwiadient descent algorithm by considering the exponentially weighted average of the gradients.\n",
    "\n",
    "# Other types of GD:\n",
    "- Adagrad (Adaptive gradient)\n",
    "   - makes the learning rate dimension specific\n",
    "   - entire learning rate is also decreasing as the algorithm proceeeds\n",
    "   - does extremley well for convex optimization\n",
    "- RMSProp\n",
    "   - very popular\n",
    "   - Never published\n",
    "   - decays in such a way that, adaptive learning are not too aggressive.\n",
    "- Adam\n",
    "   - RMSprop + SGD with momentum\n",
    "   - very fast, but the price is finding worse local minima.\n",
    "   - but moslty the go to technqiue.\n",
    "\n",
    "# Weight initialization and its techniques\n",
    "  - weights should not be 0 (will comprimize I/O connections\n",
    "  - weights should not be large \n",
    "  - weight should be normally distributed (0 to 1) and then multiply with 0.01. (this is acceptable).\n",
    "  - Xavier initialization\n",
    "     - should be randomly chose between  (-1/sqrt(n),1/sqrt(n)), where n is number of nodes in the previous layers. (this was the inital approach)\n",
    "     - but as of today, it is just N*(0,(1/n^(l-1))), where n is the number of layers in the previous layers.\n",
    "\n",
    "# Regularization\n",
    "- Overfitting and underfitting must be avoided.\n",
    "- More complex the model, more the overfit can happen.\n",
    "- It is basically preventing the model to overfit to the data (also called as shrinkage).\n",
    "- 2 techniques:\n",
    "   - L1 (lasso): Minimize the loss function over a choice of w (plus penalty). can make some of the weights 0, loosing connection (decreasing complexity)\n",
    "   - L2 (Ridge): makes the weight more less (not 0 though).    Outperforms lasso in terms of predictability. More popular than lasso\n",
    "   \n",
    "- 3 rd technique: Data augmentation:\n",
    "   - Similar to oversampling.\n",
    "   - Involves:\n",
    "      - Adding noise to the image\n",
    "      - rotate the image etc. (limited degree of rotation (but \"6\" rotated 180 might look like 9 and that is misleading)\n",
    "      - scaling\n",
    "      - cropping\n",
    "      - mirroring\n",
    "      - color adjustment\n",
    "      - even through GAN, the training data can be geenrated.\n",
    "      - Data augmentation is pretty good for voice and images.\n",
    "\n",
    "# Dropout\n",
    "- Some nodes are better predictors and some are not.\n",
    "- So during training, you remove the nodes which are better predictors and just train the weak nodes (in the process making them stronger).\n",
    "- And there is also an other combination where you train a weak node and a strong node to create a balanced combo.\n",
    "- For example, during training (due to dropout), a node might be trained based on just 2 inputs. But during testing, there might be 4 inputs (which was not trained), so the weights can be halved (multiplied with 0.5) to deal witht this. This is called simple scaling.\n",
    "- Dropout is similar to ensemble method, where some neurons are considered and some are not. other names are vanilla dropout.\n",
    "- Dense layers works great on 0.5 scaling\n",
    "- Input layers works great on 0.8 scaling\n",
    "\n",
    "\n",
    "# Batch Normalization\n",
    "- Powerful regularization technique\n",
    "- During weigh initialization and scaling, a node is given different inputs over the course of training (to a point it might actually get confused on what it is trying to learn). This is called covariate shifting.\n",
    "- So how do we solve this ?:\n",
    "   - Basically the output of a node (before or after activation function) is normalized\n",
    "   - This brings stability to the network.\n",
    "   - and this is kind of a regularization technique because it does avoid overfitting.\n",
    "     - How ?: A data point is given to different nodes from left to right. Each node has its own mean and standard deviation, so the way the data point is normalized depends on the data fed to that neuron. This is in a way is data augmentation (since the same datapoint is manipulated in a different way).\n",
    "   - But this can have a side effect during testing (or during deployment). Because during testing, there is no batch, but an arbitrary input is given.\n",
    "   - So for the above issue, we need to calculate mean_pop and standard_deviation_pop (which can be estimated from the testing data). and use that as an indicator on how the model might perform.\n",
    "   \n",
    "\n",
    "# Types of Neural network\n",
    "- Feed Forward:\n",
    "  - MLP (Multi layer perceptron)\n",
    "  - CNN\n",
    "  - DNN (with many dense layers)\n",
    "- RNN (Recurrent Neural networks)\n",
    "    - LSTM (Long short term memory NNs)\n",
    "\n",
    "# CNN\n",
    "- a feed forward network\n",
    "- used a lot in computer vision, NLP\n",
    "- For example, given an image. The ROI of the image are processed by different neurons and they overlap too. Meaning a ROI will be learnt by several neurons (of the same layer). So during inference, if only one neuron is fired for a layer, then the whole layer fires off too (leading to +ve prediction).\n",
    "\n",
    "# RNN\n",
    "- remembers weights etc from the previous iteration (which has the input from the previous iteration\n",
    "- not clear yet\n",
    "\n",
    "# GAN (Generalized Adversial networks)\n",
    "- Has 2 networks\n",
    "   - One network creates an image\n",
    "   - second network verifies the image.\n",
    "   - and it goes back in cycle, until the verification is right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing Gradient Descent\n",
    " - If the weights initilaized are very less. Then when it gets to output layer, that particular neuron has very less negligible prediction and it won't be considered.\n",
    " - so in wx+bias. w will become very less (almost 0) in the end. it will become just bias, so there will be nothing to learn.\n",
    "\n",
    "# Exploding gradient descent:\n",
    " - If the weights initialized are high, then in the output layer, that weight will be very high. and that introduces a partiality in the network.\n",
    " - **Relu and leaky Relu activation function are robust to exploding/vanishing gradient descent issue**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
